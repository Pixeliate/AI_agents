{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource : https://huggingface.co/learn/agents-course/en/unit2/llama-index/llama-hub\n",
    "\n",
    "Environment : agent_venv\n",
    "\n",
    "Libraries : llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-huggingface-api\n",
      "  Downloading llama_index_llms_huggingface_api-0.4.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.5.2-py3-none-any.whl.metadata (767 bytes)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./agent_venv/lib/python3.13/site-packages (from llama-index-llms-huggingface-api) (0.29.3)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.0 (from llama-index-llms-huggingface-api)\n",
      "  Downloading llama_index_core-0.12.28-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in ./agent_venv/lib/python3.13/site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./agent_venv/lib/python3.13/site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./agent_venv/lib/python3.13/site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./agent_venv/lib/python3.13/site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (6.0.2)\n",
      "Requirement already satisfied: requests in ./agent_venv/lib/python3.13/site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./agent_venv/lib/python3.13/site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./agent_venv/lib/python3.13/site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (4.13.0)\n",
      "Collecting aiohttp (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading aiohttp-3.11.16-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading sqlalchemy-2.0.40-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting banks<3.0.0,>=2.0.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading banks-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./agent_venv/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (1.2.18)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpx (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./agent_venv/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./agent_venv/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (3.4.2)\n",
      "Collecting nltk>3.8.1 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in ./agent_venv/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (2.2.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./agent_venv/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (11.1.0)\n",
      "Collecting pydantic>=2.8.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading pydantic-2.11.2-py3-none-any.whl.metadata (64 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: wrapt in ./agent_venv/lib/python3.13/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (1.17.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./agent_venv/lib/python3.13/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.50.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./agent_venv/lib/python3.13/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.6.0)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading scipy-1.15.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading multidict-6.3.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading yarl-1.19.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (71 kB)\n",
      "Collecting griffe (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading griffe-1.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jinja2 in ./agent_venv/lib/python3.13/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (3.1.6)\n",
      "Requirement already satisfied: platformdirs in ./agent_venv/lib/python3.13/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (4.3.7)\n",
      "Requirement already satisfied: click in ./agent_venv/lib/python3.13/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (8.1.8)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./agent_venv/lib/python3.13/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (2024.11.6)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading pydantic_core-2.33.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./agent_venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./agent_venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./agent_venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./agent_venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface-api) (2025.1.31)\n",
      "Collecting greenlet>=1 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading greenlet-3.1.1-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: setuptools in ./agent_venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./agent_venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./agent_venv/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./agent_venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./agent_venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./agent_venv/lib/python3.13/site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api) (3.0.2)\n",
      "Downloading llama_index_llms_huggingface_api-0.4.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.5.2-py3-none-any.whl (8.9 kB)\n",
      "Downloading llama_index_core-0.12.28-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
      "Downloading aiohttp-3.11.16-cp313-cp313-macosx_11_0_arm64.whl (453 kB)\n",
      "Downloading banks-2.1.1-py3-none-any.whl (28 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading pydantic-2.11.2-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.40-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp313-cp313-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl (50 kB)\n",
      "Downloading greenlet-3.1.1-cp313-cp313-macosx_11_0_universal2.whl (272 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.3.2-cp313-cp313-macosx_11_0_arm64.whl (35 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading yarl-1.19.0-cp313-cp313-macosx_11_0_arm64.whl (94 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading griffe-1.7.2-py3-none-any.whl (129 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: filetype, dirtyjson, typing-inspection, threadpoolctl, tenacity, SQLAlchemy, sniffio, scipy, pydantic-core, propcache, mypy-extensions, multidict, marshmallow, joblib, h11, greenlet, frozenlist, colorama, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, tiktoken, scikit-learn, pydantic, nltk, httpcore, griffe, anyio, aiosignal, httpx, dataclasses-json, banks, aiohttp, llama-index-core, sentence-transformers, llama-index-llms-huggingface-api, llama-index-embeddings-huggingface\n",
      "Successfully installed SQLAlchemy-2.0.40 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 banks-2.1.1 colorama-0.4.6 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 frozenlist-1.5.0 greenlet-3.1.1 griffe-1.7.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 joblib-1.4.2 llama-index-core-0.12.28 llama-index-embeddings-huggingface-0.5.2 llama-index-llms-huggingface-api-0.4.1 marshmallow-3.26.1 multidict-6.3.2 mypy-extensions-1.0.0 nltk-3.9.1 propcache-0.3.1 pydantic-2.11.2 pydantic-core-2.33.1 scikit-learn-1.6.1 scipy-1.15.2 sentence-transformers-4.0.2 sniffio-1.3.1 tenacity-9.1.2 threadpoolctl-3.6.0 tiktoken-0.9.0 typing-inspect-0.9.0 typing-inspection-0.4.0 yarl-1.19.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct (Request ID: Root=1-67f4866d-6fbc1a3840808dd60f42a92e;ae40a4ae-cb8a-4b4c-bc91-9cefbb14bf8f)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai_agent/agent_venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai_agent/agent_venv/lib/python3.13/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      9\u001b[39m hf_token = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m llm = HuggingFaceInferenceAPI(\n\u001b[32m     12\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-Coder-32B-Instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     temperature=\u001b[32m0.7\u001b[39m,\n\u001b[32m     14\u001b[39m     max_tokens=\u001b[32m100\u001b[39m,\n\u001b[32m     15\u001b[39m     token=hf_token,\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello, how are you?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# I am good, how can I help you today?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai_agent/agent_venv/lib/python3.13/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai_agent/agent_venv/lib/python3.13/site-packages/llama_index/llms/huggingface_api/base.py:306\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.complete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m formatted:\n\u001b[32m    303\u001b[39m     prompt = \u001b[38;5;28mself\u001b[39m.completion_to_prompt(prompt)\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletionResponse(\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     text=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sync_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai_agent/agent_venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:2356\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2331\u001b[39m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[32m   2332\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2333\u001b[39m             prompt=prompt,\n\u001b[32m   2334\u001b[39m             details=details,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2354\u001b[39m             watermark=watermark,\n\u001b[32m   2355\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2356\u001b[39m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[32m   2359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai_agent/agent_venv/lib/python3.13/site-packages/huggingface_hub/inference/_common.py:410\u001b[39m, in \u001b[36mraise_text_generation_error\u001b[39m\u001b[34m(http_error)\u001b[39m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttp_error\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai_agent/agent_venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:2326\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2324\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[32m   2325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2326\u001b[39m     bytes_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2328\u001b[39m     match = MODEL_KWARGS_NOT_USED_REGEX.search(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai_agent/agent_venv/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:321\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai_agent/agent_venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:481\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct (Request ID: Root=1-67f4866d-6fbc1a3840808dd60f42a92e;ae40a4ae-cb8a-4b4c-bc91-9cefbb14bf8f)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve HF_TOKEN from the environment variables\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "response = llm.complete(\"Hello, how are you?\")\n",
    "print(response)\n",
    "# I am good, how can I help you today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
